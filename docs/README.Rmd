---
title: "Alternating Manifold Proximal Gradient Method"
author: "Shixiang Chen, Justin Huang, Benjamin Jochem, Lingzhou Xue, Hui Zou"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

\newcommand{\argmin}{\operatorname{arg\,min}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, python.reticulate=FALSE)
```

- [Introduction](#introduction)
  - [Algorithm](#algorithm-description)
  - [Convergence](#convergence)
  - [Pseudocode](#pseudocode)
- [Installation](#installation)
- [Documentation](#documentation)
	- [R Usage](#r-usage)
	- [Python Usage](#python-usage)
	- [Arguments](#arguments)
	- [Values](#values)
- [Quick Start](#quick-start)
	- [R Quick Start](#r-example)
	- [Python Example](#python-example)
- [References](#references)

## Introduction

`sparsepca` and `amanpg` find sparse loadings in principal component analysis (PCA) via an alternating manifold proximal gradient method (A-ManPG). Seeking a sparse basis allows the leading principal components to be easier to interpret when modeling with high-dimensional data. PCA is modeled as a regularized regression problem under the elastic net constraints (linearized L1 and L2 norms) in order to induce sparsity. Due to the nonsmoothness and nonconvexity numerical difficulties, A-ManPG is implemented to guarantee convergence. 

The package provides a function for performing sparse PCA and a function for normalizing data.

The authors of A-ManPG are Shixiang Chen, Shiqian Ma, Lingzhou Xue, and Hui Zou. The Python and R packages are maintained by Justin Huang and Benjamin Jochem. A MATLAB implementation is maintained by Shixiang Chen.

#### Algorithm Description

The A-ManPG algorithm can be applied to solve the general manifold optimization problem

\begin{equation}
  \text{min}~F(A,B) := H(A,B)+f(A)+g(B)\text{ subject to (s.t.) }A\in\mathcal{M}_1, B\in\mathcal{M}_2
\end{equation}

where $H(A,B)$ is a smooth function of $A,B$ with a Lipschitz continuous gradient, $f(\cdot)$ and $g(\cdot)$ are lower semicontinuous (possibly nonsmooth) convex functions, and $\mathcal{M}_1,\mathcal{M}_2$ are two embedded submanifolds in the Euclidean space.

For sparse PCA, the following function definitions are used:

- $H(A,B)=\text{Tr}(B^TX^TXB)-2\text{Tr}(A^TX^TXB)$
- $f(A)\equiv 0$
- $g(B)=\lambda_2\sum_{j=1}^k\|B_j\|_2^2+\sum_{j=1}^k\lambda_{1,j}\|B_j\|_1$
- $\mathcal{M}_1=\text{St}(p,k)$
- $\mathcal{M}_2=\mathbb{R}^{p\times k}$ 

where $X$ is the $n\times p$ data matrix or $n\times n$ covariance matrix, $A$ is the scores and $B$ is the loadings, $k$ is the rank of the matrices (in other words, how many principal components are desired), $\lambda_1$ is the L1 norm penalty and $\lambda_2$ is the L2 norm penalty. Both the L1 and L2 norm are used as elastic net regularization to impose sparseness within the loadings. Note that a different L1 norm penalty is used for every principal component, and the algorithm operates differently when the L2 norm penalty is set to a large constant (`np.inf` or `Inf`).

The A-ManPG algorithm uses the following subproblems with an alternating updating scheme to solve sparse PCA, computed in a Gauss-Seidel manner for faster convergence.

\begin{align}
  D_k^A := \argmin_{D^A} \langle\nabla_A H(A_k,B_k),D^A\rangle + f(A^k + D^A) + \frac{1}{2t_1}\|D^A\|^2_F~\text{ s.t. }~D^A\in\text{T}_{A_k}\mathcal{M}_1 \\
  D_k^B := \argmin_{D^B} \langle\nabla_A H(A_{k+1},B_k),D^B\rangle + f(B^k + D^B) + \frac{1}{2t_2}\|D^B\|^2_F~\text{ s.t. }~D^B\in\text{T}_{B_k}\mathcal{M}_2
\end{align}

where $A_{k+1}$ is obtained via a retraction operation (in this case, polar decomposition), $t_1\leq L_A$, and $t_2\leq L_B$. $L_A$ and $L_B$ are the least upper bounds of the Lipschitz constants for $\nabla_A H(A,B)$ and $\nabla_B H(A,B)$, respectively. The subproblems are solved using an adaptive semismooth Newton method.

#### Convergence

Let $\epsilon$ represent a tolerance level to detect convergence. An *$\epsilon$-stationary point* is defined as a point $(A, B)$ with corresponding $D^A$ and $D^B$ that satisfy the following:

\begin{equation}
  \|D^A/t_1\|_F^2+\|D^B/t_2\|_F^2 \leq \epsilon^2
\end{equation}

The algorithm reaches an 
$\epsilon$-stationary point in at most

$$\frac{2(F(A_0,B_0)-F^*)}{ ((\gamma\bar{\alpha}_1t_1+\gamma\bar{\alpha}_2t_2)\epsilon^2)}$$

iterations, where:

- $(A_0,B_0)$ are the initial values
- $F^*$ is the lower bound of $F$, from the general manifold optimization problem
- $\bar{\alpha}_1$ and $\bar{\alpha}_2$ are positive constants

#### Pseudocode

The following describes the algorithm used for solving the general manifold optimization problem using A-ManPG. For solving sparse PCA, the algorithm is implemented with the aforementioned definitions.

```{r tidy=FALSE, eval=FALSE, highlight=FALSE}
Input initial point (A0,B0) and necessary parameters for the required problem 

for i=0,1,... do
  Solve the first subproblem for Da
  Set alpha = 1
  
  while F(Retr(alpha * Da),B) > F(A,B) - alpha * norm(Da)^2 / (2 * t1) do
    alpha = gamma * alpha
  end while
  
  Set A = Retr(alpha * Da)
  
  Solve the second subproblem for Db
  Set alpha = 1
  
  while F(A,Retr(alpha * Db)) > F(A,B) - alpha * norm(Db)^2 / (2 * t2) do
    alpha = gamma * alpha
  end while
  
  Set B = Retr(alpha * Db)
end for

Return A as the scores and B as the sparse loadings
```

## Installation

To install the R package, install `amanpg` directly from CRAN.
```{r eval=FALSE, highlight=FALSE}
install.packages("amanpg")
```

To install the Python package, use `pip` to obtain `sparsepca` from PyPI.
```{python eval=FALSE, highlight=FALSE}
pip3 install sparsepca
```

## Documentation

#### R Usage

```{r eval=FALSE, highlight=FALSE}
spca.amanpg(z, lambda1, lambda2, 
			f_palm = 1e5, x0 = NULL, y0 = NULL, k = 0, type = 0, 
			gamma = 0.5, maxiter = 1e4, tol = 1e-5, 
			normalize = TRUE, verbose = FALSE)
```

#### Python Usage

```{python eval=FALSE, highlight=FALSE}
spca(z, lambda1, lambda2, 
     x0=None, y0=None, k=0, gamma=0.5, type=0, 
     maxiter=1e4, tol=1e-5, f_palm=1e5,
	 normalize=True, verbose=False):
```

#### Arguments

| Name | Python Type | R Type | Description |
| --- | --- | --- | --- |
| `z` | numpy.ndarray | matrix | Either the data matrix or sample covariance matrix |
| `lambda1` | float list | numeric vector | List of parameters of length n for L1-norm penalty |
| `lambda2` | float or numpy.inf | numeric or Inf | L2-norm penalty term |
| `x0` | numpy.ndarray | matrix | Initial x-values for the gradient method, default value is the first n right singular vectors |
| `y0` | numpy.ndarray | matrix | Initial y-values for the gradient method, default value is the first n right singular vectors |
| `k` | int | int | Number of principal components desired, default is 0 (returns min(n-1, p) principal components) |
| `gamma` | float | numeric | Parameter to control how quickly the step size changes in each iteration, default is 0.5 |
| `type` | int | int | If 0, b is expcted to be a data matrix, and otherwise b is expected to be a covariance matrix; default is 0 |
| `maxiter` | int | int | Maximum number of iterations allowed in the gradient method, default is 1e4 |
| `tol` | float | numeric | Tolerance value required to indicate convergence (calculated as difference between iteration f-values), default is 1e-5 |
| `f_palm` | float | numeric | Upper bound for the F-value to reach convergence, default is 1e5 |
| `normalize` | bool | logical | Center and normalize rows to Euclidean length 1 if True, default is True |
| `verbose` | bool | logical | Function prints progress between iterations if True, default is False |e

#### Values

Python returns a dictionary with the following key-value pairs, while R returns a list with the following elements:

| Key | Python Value Type | R Value Type | Value |
| --- | --- | --- | --- |
| `loadings` | numpy.ndarray | matrix | Loadings of the sparse principal components |
| `f_manpg` | float | numeric | Final F-value |
| `x` | numpy.ndarray | matirx | Corresponding ndarray in subproblem to the loadings |
| `iter` | int | numeric | Total number of iterations executed |
| `sparsity` | float | numeric | Number of sparse loadings (`loadings == 0`) divided by number of all loadings |
| `time` | float | numeric | Execution time in seconds |

## Quick Start

Consider the two examples below for running sparse PCA on randomly-generated data: one using a finite $\lambda_2$, and the other using a large constant $\lambda_2$.

#### R Example

In the following example, we load the library using `library(amanpg)` and then define a 1000x500 randomly-generated matrix from the normal distribution. We set the L1-penalty term to 0.1 and L2-penalty term to infinity, and seek the first four principal components.

The default initial point are the `k` right singular vectors from SVD, which we can see manually broken down here. In the function call, we pass the parameters in and output our list sprout. 

The results are printed out, and then we view the loadings.

```{r}
d <- 500  # dimension
m <- 1000 # sample size
a <- normalize(matrix(rnorm(m * d), m, d))
lambda1 <- 0.1 * matrix(data=1, nrow=4, ncol=1)
x0 <- svd(a, nv=4)$v
sprout <- spca.amanpg(a, lambda1, lambda2=Inf, x0=x0, y0=x0, k=4) 
print(paste(sprout$iter, "iterations,", sprout$sparsity, "sparsity,", sprout$time))

# extract loadings
View(sprout$loadings)
```

#### Python Example

Note that the Python package depends on numpy.

In the following example, the package function is imported first. The appropriate parameters are defined&mdash;in this case, we want four sparse principal components (rank-`k` loadings)&mdash;from a 1000x500 data matrix. The L1-penalty terms are set to 0.1, and the L2-penalty term is set to 1. Note that any positive value can be used for the L2-penalty term, up to `np.inf`. 

A random 1000x500 matrix is generated from the normal distribution, and then the function is called through `spca()`. A printout of the results follows, along with observing the loadings. 

The second example keeps the same parameters except switching `lambda2` with infinity. Again, the results are printed out and the loadings are observed.

```{python reticulate=FALSE}
import numpy as np
from sparsepca import spca

k = 4  # columns
d = 500  # dimensions
m = 1000  # sample size
lambda1 = 0.1 * np.ones((n, 1))
lambda2 = 1

np.random.seed(10)
a = np.random.normal(0, 1, size=(m, d))  # generate random normal 1000x500 matrix
fin_sprout = spca(a, lambda1, lambda2, k=k)
print(f"Finite: {fin_sprout['iter']} iterations with final value 
		{fin_sprout['f_manpg']}, sparsity {fin_sprout['sparsity']}, 
		timediff {fin_sprout['time']}.")

fin_sprout['loadings']

inf_sprout = spca_amanpg(a, lambda1, np.inf, k=4)
print(f"Infinite: {inf_sprout['iter']} iterations with final value 
		{inf_sprout['f_manpg']}, sparsity {inf_sprout['sparsity']}, 
		timediff {inf_sprout['time']}.")

inf_sprout['loadings']
```

## References

Chen, S., Ma, S., Xue, L., and Zou, H. (2020) "An Alternating Manifold Proximal Gradient Method for Sparse Principal Component Analysis and Sparse Canonical Correlation Analysis" INFORMS Journal on Optimization 2:3, 192-208 <[doi:10.1287/ijoo.2019.0032](https://doi.org/10.1287%2Fijoo.2019.0032)>.

Zou, H., Hastie, T., & Tibshirani, R. (2006). Sparse principal component analysis. Journal of Computational and Graphical Statistics, 15(2), 265-286 <[doi:10.1198/106186006X113430](https://doi.org/10.1198%2F106186006X113430)>.

Zou, H., & Xue, L. (2018). A selective overview of sparse principal component analysis. Proceedings of the IEEE, 106(8), 1311-1320 <[doi:10.1109/JPROC.2018.2846588](https://doi.org/10.1109%2FJPROC.2018.2846588)>.

